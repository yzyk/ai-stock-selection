# Data Module: A combination of decorator pattern and factory pattern

With the newly proposed improved asset return prediction neural network, the variance for demands on different kinds of data and data input pipeline gets increased, thus raising importance and necessity to extend the existing data module.

## Analysis on demands

Now let us first do an analysis on demands of data and data input pipeline. The demand is constant in the following aspects:
* It expects to read stock data
* It expects to output standardized train, validation and test data, that should be called by model and portfolio development modules

The changing aspects of data and data input pipeline can be concluded as the following dimensions:
* Underlying data structure and algorithm: the data can be implemented in different ways, based on different data structure, generated by different algorithms
* Specific content of data, which can be further divided into:
    * whether we are looking back and predicting forward for multiple time steps, the answer is yes or no determines totally different implementation
    * different combinations of linear or element-wise data transformation, different combinations of transformation results in different combinations of methods or classes
    * which kind of shape we want the data to be, different shape may leads to totally different implementations for certain data structures
    
## Design</u></font>

After an overwhelming research, we decide to design an architecture combing interface oriented programming concepts, together with well-known decorator pattern and factory method pattern for the data module.

<img src="./img/Data.png" alt="framework" title="framework" width="40000" height="800"/>

<u>**Interface oriented programming**</u>

This is a more Java concepts. Here in python by interface, we define an abstract class `Data`, whose member methods are mostly abstract and wait for its subclasses to implement. As the highest and the most abstract level of this architecture, it only defines a few methods to output the constant aspects of data and data input pipeline: it expects to output standardized train, validation and test data, that should be called by model and portfolio development modules.

```python
class Data(ABC):

    def __init__(self) -> None:
        pass

    @abstractmethod
    def get_train(self):
        return None

    @abstractmethod
    def get_val(self):
        return None

    @abstractmethod
    def log(self):
        pass

    @abstractmethod
    def generate(self, *args):
        pass

    @abstractmethod
    def X_test(self):
        return None

    @abstractmethod
    def ids_test(self):
        return None
```

We then use single inheritance to derive new subclasses of this abstract class. These subclasses differ in the way of implementation, i.e. the data structure they rely on and the algorithm they use to generate data. As mentioned above, we anticipate a large consumption of memory space, so besides the conventional way to read data in memory and transform them in memory into what we want and feed them into the model, we also implement a more memory efficient class based on generator in expectation of consuming as little memory space as we can.

```python

# implementation based on numpy array
class InMemoryData(Data, ABC):...


# implementation based on generator
class GeneratedData(Data, ABC):...
  
```


<u>**Decorator pattern**</u>

Another variance of demands in the future is classified as data's content. We realize that data's content can change in combinations of multiple dimensions, and if we use classes to represent each dataset after each combination of these dimensions, the number of classes that we have to maintain can easily explode. We hereby choose [decorator pattern](https://en.wikipedia.org/wiki/Decorator_pattern) to implement this aspect, to avoid creating too many concrete classes. To summarize, the decorator pattern solves the following problem:
* Functionalities and behavior (here refers to the transformation of data) should be added dynamically into the base concrete class object
* A flexible way of combining potential adding of behavior should be provided, rather than subclass every possible object after every such combination

Note while the decorator pattern is flexible, it is still limited by the underlying data structure of the class it is decorating. This means each implementation of class `Data` should have a corespondent group of decorators. Hence, we define two abstract decorator class for the two subclasses of `Data`. We define these two groups of decorators with the same signature in two different modules, so that we can write codes with the same signature to call decorators but defer the decision of which module to call to runtime.

```python

# InMemoryDataRecipe.py
# decorators for implementation based on numpy array
class DataDecorator(InMemoryData, ABC):

    def __init__(self, data):
        super().__init__()
        self.copy_constructor(data)
        self.generate()
        pass

    @abstractmethod
    def generate(self):
        pass
    

# GeneratedDataRecipe.py
# decorators for implementation based on generator
class DataDecorator(GeneratedData, ABC):

    def __init__(self, data):
        super().__init__()
        self.copy_constructor(data)
        self.generate()
        pass

    @abstractmethod
    def generate(self):
        pass
```

In the design of decorator pattern, we also need to provide a concrete data class for decorators to decorate. We can choose any stage within the data's transformation, but we here choose the very first stage, which is when the data gets read into the memory space from the disk.

```python

class DiskData(InMemoryData):...

class DiskData(GeneratedData):...
```

Within each module, the `DataDecorator` are subclassed to implement concrete decorators.


```python

'''
In Memory Data Decorators
'''


class InMemoryDataDecorator(DataDecorator, ABC):

    def generate(self):
        self.in_memory_data_process()
        pass

    @abstractmethod
    def in_memory_data_process(self):
        pass


class PreSplitScaler(InMemoryDataDecorator):...

'''
Pre Split Decorators
'''


class PreSplitDataDecorator(DataDecorator, ABC):

    def generate(self):
        self.pre_split_data_process()
        pass

    @abstractmethod
    def pre_split_data_process(self):
        pass


class WindowGenerator(PreSplitDataDecorator):...


'''
Split Decorators
'''


class SplitDataDecorator(DataDecorator, ABC):

    def generate(self):
        self.split_data_process()
        pass

    @abstractmethod
    def split_data_process(self):
        pass


class StandardSplitProcessor(SplitDataDecorator):...


'''
Post Split Decorators
'''


class PostSplitDataDecorator(DataDecorator, ABC):

    def generate(self):
        self.post_split_data_process()
        pass

    @abstractmethod
    def post_split_data_process(self):
        pass


class FirstDimensionReshaper(PostSplitDataDecorator, ABC):...
    

class FirstDimensionReshaperOnUnWindowed(FirstDimensionReshaper):...


class FirstDimensionReshaperOnWindowed(FirstDimensionReshaper):...


class TestDataHandlerOnFirstDimensionAsRecord(PostSplitDataDecorator):...


class TestDataHandlerOnFirstDimensionAsDate(PostSplitDataDecorator):...
```


<u>**Factory method pattern**</u>

While decorator pattern can avoid as many subclassing as we can, it however, causes inconvenience in the client side. A client must now very well which combination of decorators from data recipes he must explicitly write into the codes to get the data he wants. We hereby use [factory method pattern](https://en.wikipedia.org/wiki/Factory_method_pattern) to encapsulate the process of producing concrete data, and to provide unified and standard data products.

We first define an abstract class `DataProcessor` an interface factory for all factory subclasses. We then subclass and override the abstract method to define which combination of decorators we are choosing to produce the data. In this way, the client only cares about which factory he is calling, where the class name consist of specific dimensions and shapes for this data.

```python
class DataProcessor(ABC):
    def __init__(self, train_start, val_start, test_start, test_end, generator=False, *args):
        if generator:
            self._module = GeneratedDataRecipe
        else:
            self._module = InMemoryDataRecipe
        self._train_start = train_start
        self._val_start = val_start
        self._test_start = test_start
        self._test_end = test_end
        pass

    def load_data(self, *args):
        data = self._load_data(*args)
        data.log()
        return data

    @abstractmethod
    def _load_data(self, *args):
        pass
    
    
class TwoDimUnWinDataByStockDateByFeatureProcessor(DataProcessor):...


class ThreeDimUnWinDataByDateByStockByFeatureProcessor(DataProcessor):...

    
class ThreeDimWinDataByStockDateByWinByFeatureProcessor(DataProcessor):...


class FourDimWinDataByDateByWinByStockByFeatureProcessor(DataProcessor):

    def __init__(self, train_start, val_start, test_start, test_end, win_size, forward_size, generator=False, *args):
        self._win_size = win_size
        self._forward_size = forward_size
        super().__init__(train_start, val_start, test_start, test_end, generator)

    def _load_data(self):
        data = self._module.DiskData(self._train_start, self._val_start, self._test_start, self._test_end)
        data = self._module.PreSplitScaler(data)
        data = self._module.WindowGenerator(data, self._win_size, self._forward_size)
        data = self._module.StandardSplitProcessor(data)
        data = self._module.FirstDimensionReshaperOnWindowed(data)
        data = self._module.TestDataHandlerOnFirstDimensionAsDate(data)
        return data

```

<u>**Data Structure**</u>

The abstract class `Data` is implemented by the following two subclasses:
* `InMemoryData`: 
    * is based on data structure `numpy.ndarray`
    * reads disk data directly into memory space and apply transformation on the whole piece of memory data, finally feed the whole memory data into the model
* `GeneratedData`
    * is based on self-defined data structure `ArrayGenerator`
    * reads disk data directly into memory space 
    * when calling by model, use `ArrayGenerator` object to iterate through sub and small pieces of memory data and apply transformation on this small piece of memory data, which result is then forwarded into model
    
The `GeneratedData` is developed to solve the heavy consumption on memory space of training data. If we exclude the data originally read in, then the memory space `InMemoryData` objects consume is $O(N)$, where $N$ denotes the number of records/samples, whereas the memory space `GeneratedData` objects consume is $O(1)$.

We introduce the following skeleton for the underlying data structure `ArrayGenerator` for `GeneratedData`. An `ArrayGenerator`: object needs the following component to generate data:
* `batch size`: denotes how many samples are we generating each time. While generating one sample each time consumes the least memory as possible, it may take too long time for model training. We hereby set this to be a variable that can be configured and tuned
* `_memory_data`: the base data stored in memory that we are iterating through and generating data from
* `_memory_data_index_range`: the list of available indexes of the base data `_memory_data` that we can iterating through. Note this cannot always be `range(_memory_data.shape[0])`, for some operations and transformations such as generating data window may make some of these indexes (records) not legitimate any more
* `_cur_indexes_on_memory_data_index`: the index list of `_memory_data_index_range` that we are generating from in the current step. This should be an `numpy.ndarray` as we do increment operation on it. Due to the possibility of reshaping, this list doesn't necessarily have only one element
* `_steps`: the number used to increment `_cur_indexes_on_memory_data_index` each iteration. It should be set to 1 if we don't want to skip anything
* `_count`: how many iteration we have gone so far
* `_limit`: how many iterations we can go
* `_last_mod`: the `batch size` may not cover all the samples equally, so we need a mod to adjust the batch size output by the last iteration
* `_ops_funcs`: list to store the operations/functions a generator should perform to output its data

Suppose we have already set all variables, when called, an `ArrayGenerator` object will
* determine a list of indexes to slice `_memory_data` based on `_cur_indexes_on_memory_data_index` and `_memory_data_index_range`
* slice data with the determined indexes from `_memory_data`
* forward this data into each function in `_ops_funcs` sequentially
* update the `_cur_indexes_on_memory_data_index` by `_steps` to prepare for the next iteration
* return the data

Hence, at the data prerprocessing stage before feeding data into model, what each `DataDecorator` for `GeneratedData` do is configure the variables mentioned above, so that `ArrayGenerator` know where to find and how to compute when it is called by the model. This is totally different from what we do to the whole memory data, where we do all computation at first then forward the processed data into model.

`GeneratedData` is hereby, an implementation based on `ArrayGenerator`, where most of its member variable are set to be `ArrayGenerator`. It also provides wrapper functions to convert `ArrayGenerator` into `tf.data.Dataset` readable by TensorFlow.
    
```python
class ArrayGenerator(Generator):

    def __init__(self, memory_data,
                 memory_data_index_range=None, data_shape=None,
                 ops_funcs=None, start_indexes_on_index=None):
        self.batch_size = ...
        self._memory_data = ...
        self._memory_data_index_range = ...
        self._data_shape = ...
        self._cur_indexes_on_memory_data_index = ...
        self._steps = ...
        self._count = 0
        self._limit = ...
        self._last_mod = ...
        self._ops_funcs = []
        super().__init__()
        self._start_indexes_on_indexes = ...
        pass

    def _compute(self, data):...

    def reset(self):...

    def send(self, ignored_arg):...

    def throw(self, type=None, value=None, traceback=None):...
        
    def update_memory_data(self, memory_data):...

    def update_indexes(self, indexes):...

    def update_index_range(self, index_range):...

    def add_ops_funcs(self, func):...
        
    def update_shape(self, shape):...

    def update_limit(self, limit):...

    def clear_ops_funcs(self):...

    def update_batch_size(self, size):...

    @property
    def shape(self):...

    @property
    def memory_data_index_range(self):...

    @property
    def cur_indexes_on_memory_data_index(self):...

    @property
    def ops_funcs(self)...

    def generate_all(self):...
       
```

        
<u>**Details of data decorators**</u>

As presented before, `InMemoryData` and `GeneratedData` both have fully implemented `DataDecorators` to perform data preprocessing. A full list of these `DataDecorators` are:

* Pre Split Decorators:
    * `PreSplitScaler` 
    * `WindowGenerator`
* Split Decorators:
    * `StandardSplitProcessor`
* Post Split Decorators:
    * `FirstDimensionReshaperOnUnWindowed`
    * `FirstDimensionReshaperOnWindowed`
    * `TestDataHandlerOnFirstDimensionAsRecord`
    * `TestDataHandlerOnFirstDimensionAsDate`
    
Here we only illustrate some important decorators.

`WindowGenerator` converts the current data into time series data, where we can collect all features during a past period within the given `_data_window_size` and predict all labels during a future period within the given `_forward_steps_size`. We should skip a number of very first samples for each ticker because they don't contain sufficient information.
* For `InMemoryData`, we only write a for loop to iteratively update features for each record to contain the look back window and update labels for each record to contain the look forward window. We update the indexes for training, validation and test data excluding those who cannot provide sufficient information
* For `GeneratedData`, we configure the member variables of each component generator by:
    * update the `_memory_data_index_range` for training, validation and test data generators excluding those who cannot provide sufficient information
    * extract the logic in a single iteration in the for loop conducted by `InMemoryData` and append them into the `_ops_funcs` of these generators
    
`StandardSplitProcessor` split the current data into train, validation and test dataset.
* For `InMemoryData`, we perform a very simple slice data operation on `_X` and `_y` variable based on indexes for training, validation and test dataset, then assign them into correspondent member variables
* For `GeneratedData`, we configure the member variables of each component generator by:
    * Constructing new `ArrayGenerator` objects for corresponding variables, passing into them of
    * Shared memory data, correspondent index ranges, shape and accumulated operation function list
    
The two `FirstDimensionReshaper` targets on different possible input data. They both rearrange and transpose the current data by splitting the first dimension from *per stock per date* to two separate dimensions where *date* takes the first dimension and *stock* takes the second.
* For `InMemoryData`, we directly pass the current data into reshape and transpose functions then output the data
* For `GeneratedData`, we configure the member variables of each component generator by:
    * Updating the starting `_cur_indexes_on_memory_data_index` member variable, since the first dimension of the output data is different from the memory data now we have to do computation to make the indexes re-locate samples of the memory data that will be able to fully transformed into samples in output shape
    * Append the reshape and transpose function into the operation function list
    
We are fully aware of some limitations on implementation of `DataDecorator` for `GeneratedData`. While all linear and element-wise decorators can be chained for any times and in any order, some non-linear transformation is subject to specific order. For instance:
* `WindowGenerator` must be applied on the memory_data, meaning it must be placed in the first order

Some findings:
* **We have run tests and proved that `GeneratedData` outputs perfectly same data with `InMemoryData`**
* **`GeneratedData` can handle data size up to 20 years, but `InMemoryData` for 1 year crashes python interpreter on a machine with RAM of 16 GB**

## Demo and Application

To extend and create your own dataset, first identify the transformations you want. If the transformations cannot be realized by the built-in data decorators, you can build your own by subclassing the `DataDecorator` in selected recipe module. Your own data decorator is expected to directly modify the member variables of `Data` objects to implement the desired transformations. When decorators are determined, subclass the `DataProcessor` and configure the selected decorators when overriding the `_load_data()` method. This processor can then be directly called to generate data as you wish in client side or configured in factories.

```python
class MyDataDecorator(*DataRecipe.DataDecorator):

    def generate(self):
        self.my_data_process()
        pass

    def my_data_process(self):
        pass

class MyDataProcessor(DataProcessor.DataProcessor):

    def __init__(self, *args):
        # some config
        super().__init__(*args)

    def _load_data(self, *args):
        data = self._module.DiskData(*args)
        data = self._module.MyDataDecorator1(data, *args)
        data = self._module.MyDataDecorator2(data, *args)
        ...
        data = self._module.MyDataDecoratorN(dat, *args)
        return data

class MyFactory(DlFactory):

    def _load(self):
        self._data = MyDataProcessor(*args).load_data()
        self._model = some_model
        pass
```